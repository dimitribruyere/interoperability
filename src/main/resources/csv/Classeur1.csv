Localisation;Intitulé;Description;Compétences;Durée;Gratification
Laboratoir Hubert Curien;Traitement d'image et reconnaissance de structures pour le contrôle de produits industriels;L'objectif du stage est de mettre au point une méthode d'analyse et de reconnaissance d'images pour automatiser le tri des produits fabriqués. Cette méthode devra s'appuyer sur l'étude déjà réalisée et s'intégrer dans l'applicatif existant en respectant un cahier des charges. Le projet sera mené sous la forme d'un partenariat recherche-industrie avec des réunions se déroulant au sain de chaque structre. Les principales étapes envisagée concernent : -la mise au point de méthodes de détection de motifs caractétistiques et de description de forme et de texture. - L'évaluation et la validation des méthodes développées sur des jeux de données fournis  par l'industriel. - L'intégration des algorithmes dans l'applicatif existant. Les méthodes développées pourront s'appuyer sur des approches issues du tratiement d'image (filtrage, morphologie mathématique, descripteurs locaux) et de l'apprentissage automatique (apprentissage de représentation, classification automatique).;connaissance du domaine du traitement d'image/maîtrise du développement sous Matlab/capacité à synthétiser et à communiquer les résultats;6;560
Laboratoir Hubert Curien;Majority Vote as a Source of Additional Knowledge for Classification;Our goal is to study the trade-off between the level of expertize of a voter, and the potential gains to be obtained by using (experts) majority vote. More precisely, assuming that all voters have the same level of expertize (probability that an expert errs), for a given number of expert interactions, we are interested in analytically defining the level of expertize when the majority vote is well motivated. A possible approach is to use entropy based methods for iteratively proposing discriminant features to experts. Then, a model will be constructed including only the features selected by the expert. The first objective for the internship is to construct an algorithm that is able to take into account the expert feedback through an iterative procedure. In particular, the algorithm should be able to ground ground truth, with an expert user that makes no mistakes. Then, the second step is to study how the performance of the model is modified (with respect to the ground truth), when the quality of expert feedback decreases. For cases of reduced quality of feedback, we will compare the performance of one expert to the performance given by the use of a majority vote over expert feedbacks.;machine learning/ combination of experts / entropy / data mining;6;560
Laboratoir Hubert Curien;A Study of unsupervised anomaly detection and rare event detection methods;Anomaly detection consists in detecting anomalies in a set of data or in a data stream. The applications are numerous and cover a wide range of domains: fault detection, medical imaging, fraud detection, intrusion detection, and many more. When the types of anomalies are known and an expert is able to annotate them, the problem of anomaly detection falls back to a traditional classi_cation problem (normal vs anomaly). However, in many applications, where anomalies are rare and show a lot of variations, it becomes impossible to learn a traditional classi_er. In such cases, semi- supervised or unsupervised anomaly detection methods must be used. These methods are at the core of this subject, which aims at drawing a detailed map of existing approaches for unsupervised anomaly detection and understanding the current state of the art of the domain.;machine learning/ combination of experts / entropy / data mining;6;560
Laboratoir Hubert Curien;Network mining using deep learning;The field of complex networks is a fairly recent scientific field, which focuses on the analysis, modeling and study of real networks, or graphs that model observable phenomena. Examples include social networks, telecommunications networks, biological networks (proteins, genes, etc.), neural networks, and so on. Recently, a particular interest is focused on dynamic networks, ie networks that evolve over time (Appearance / disappearance of nodes / links, or sequences of interactions). The classic problems in complex network analysis are the detection of communities (groups of similar nodes), the prediction of links (for example in music recommendation systems or arbitrary products), or the diffusion phenomena (diffusion of viruses or false information, for example). A new data mining technique recently introduced, called embedding (1), is attracting a very important interest from the scientific community. In two words, for elements included in a context, the embedding will plunge these elements into a space of small dimension, so that elements with similar contexts are found in positions close to this space. The most well-known application is word embeddding, in which, from any body of text, we will project words in a space that respects semantics. An interesting property is that this embedding respects certain semantic properties: each node is associated with a vector (its position in the projection space), and we can do operations on these vectors that make sense. For example, Paris - France + England will give as result London. These results are generally obtained using Deep Neural Networks (Deep Neural Networks).;Social mining / social network / graph / embedding / deep learning;6;560
